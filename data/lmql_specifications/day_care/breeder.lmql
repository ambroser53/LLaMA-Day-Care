{
    "fitness_test_template":
    '''
    {decoder_options}
        "[[INST]] <<SYS>>"
        "{system_prompt}"
        "<</SYS>>\n\n"

        "Input Text: {text}\n"

        "Question: {question}\n"

        if not CoT:
            "{task_prompt}[[/INST]][REQUIREMENT]\n" where REQUIREMENT in set([" Yes", " No"])
        else:
            "{task_prompt}[[/INST]] {reasoning_prompt} [REASONING]\n" where len(TOKENS(REASONING)) < 500
            "Final answer: [REQUIREMENT]\n" where REQUIREMENT in set([" Yes", " No"])
    from
        lmql.model({model_name_or_path}, cuda=True, batch_size={batch_size}, inprocess=True, quantization_config={frozenset(model_kwargs["quantization_config"].__dict__.items())}, dtype="bfloat16", bits=4)
    ''',
    "fitness_test_template_args": ["system_prompt", "text", "question", "task_prompt", "reasoning_prompt"],
################################################
################################################
    "mutation_direct":
    '''
    {decoder_options}
        "[[INST]] <<SYS>>"
        "You are a prompt engineer creating better prompts for specific tasks. You are to create a prompt for the task in the way specified. Each prompt has three components: the system-prompt specifies the role the model must play, the task-prompt specifies what the model is being asked to do and the reasoning-prompt specifies how the model should provide reasoning."
        "<</SYS>>\n\n"

        # MUTATION PROMPT
        "{mutation_prompt}\n"

        # THINKING STYLE
        if thinking_style:
            "{thinking_style}\n"

        "\nINSTRUCTION:\n"

        # TASK DESCRIPTION OR TASK PROMPT
        if task_description:
            "{task_description}"
        else:
            "{task_prompt}"

        "\n\nINSTRUCTION MUTANT:[[\INST]]"
        "System-Prompt: [SYSTEM_PROMPT]\n" where STOPS_BEFORE(SYSTEM_PROMPT, "\n") and STOPS_BEFORE(SYSTEM_PROMPT, "Task-Prompt:") and STOPS_BEFORE(SYSTEM_PROMPT, "Reasoning-Prompt:") and STOPS_BEFORE(SYSTEM_PROMPT, "  ") and len(TOKENS(SYSTEM_PROMPT)) < 250
        "Task-Prompt: [TASK_PROMPT]\n" where STOPS_BEFORE(TASK_PROMPT, "\n") and STOPS_BEFORE(TASK_PROMPT, "Reasoning-Prompt:") and STOPS_BEFORE(SYSTEM_PROMPT, "  ") and len(TOKENS(TASK_PROMPT)) < 250
        "Reasoning-Prompt: [REASONING_PROMPT]\n" where STOPS_AT(REASONING_PROMPT, "\n") and STOPS_BEFORE(REASONING_PROMPT, "  ") and STOPS_BEFORE(REASONING_PROMPT, "]") and len(TOKENS(REASONING_PROMPT)) < 250
    from
        lmql.model({model_name_or_path}, cuda=True, batch_size={batch_size}, inprocess=True, quantization_config={frozenset(model_kwargs["quantization_config"].__dict__.items())}, dtype="bfloat16", bits=4)
    ''',
    "mutation_direct_args": ["mutation_prompt", "thinking_style", "task_description", "task_prompt"],
################################################
    "mutation_eda_based":
    '''
    {decoder_options}
        "[[INST]] <<SYS>>"
        "You are a prompt engineer creating better prompts for specific tasks. You are to create a prompt for the task in the way specified. Each prompt has three components: the system-prompt specifies the role the model must play, the task-prompt specifies what the model is being asked to do and the reasoning-prompt specifies how the model should provide reasoning."
        "<</SYS>>\n\n"

        # MUTATION PROMPT
        "INSTRUCTION: {mutation_prompt}\n"

        # EDA METHOD BASED PROMPTING
        if eda_method == "lineage":
            "GENOTYPES FOUND IN ASCENDING ORDER OF QUALITY\n"
        elif eda_method == "ranked":
            "A List of Responses in descending order of score. " + str(len(task_prompts)) + " is the best response. It resembles" + str(len(task_prompts)-1) + "more than it does (1)\n"
        else:
            "A List of Example Responses\n"

        # TASK PROMPT
        for i, task_prompt in enumerate(task_prompts):
            "{i}: {task_prompt}\n"

        "\n\INSTRUCTION MUTANT:[[\INST]]"
        "System-Prompt: [SYSTEM_PROMPT]\n" where STOPS_BEFORE(SYSTEM_PROMPT, "\n") and STOPS_BEFORE(SYSTEM_PROMPT, "Task-Prompt:") and STOPS_BEFORE(SYSTEM_PROMPT, "Reasoning-Prompt:") and STOPS_BEFORE(SYSTEM_PROMPT, "  ") and len(TOKENS(SYSTEM_PROMPT)) < 250
        "Task-Prompt: [TASK_PROMPT]\n" where STOPS_BEFORE(TASK_PROMPT, "\n") and STOPS_BEFORE(TASK_PROMPT, "Reasoning-Prompt:") and STOPS_BEFORE(SYSTEM_PROMPT, "  ") and len(TOKENS(TASK_PROMPT)) < 250
        "Reasoning-Prompt: [REASONING_PROMPT]\n" where STOPS_AT(REASONING_PROMPT, "\n") and STOPS_BEFORE(REASONING_PROMPT, "  ") and STOPS_BEFORE(REASONING_PROMPT, "]") and len(TOKENS(REASONING_PROMPT)) < 250
    from
        lmql.model({model_name_or_path}, cuda=True, batch_size={batch_size}, inprocess=True, quantization_config={frozenset(model_kwargs["quantization_config"].__dict__.items())}, dtype="bfloat16", bits=4)
    ''',
    "mutation_eda_based_args": ["mutation_prompt", "eda_method", "task_prompts"],
################################################
    "mutation_lamarckian":
    '''
    {decoder_options}
        "[[INST]] <<SYS>>"
        "You are a prompt engineer creating better prompts for specific tasks. You are to create a prompt for the task in the way specified. Each prompt has three components: the system-prompt specifies the role the model must play, the task-prompt tells the model what to do and the reasoning-prompt specifies how the model should provide reasoning."
        "<</SYS>>\n\n"

        # MUTATION PROMPT
        "INSTRUCTION: {mutation_prompt}\n"

        # LAMARCKIAN BASED PROMPTING
        "I gave a friend an instruction and some advice. Here are the correct examples of his workings out:\n"
        for r in correct_reasonings:
            r + "\n"
        "\nThe instruction was: " + task_prompt

        "\n\nINSTRUCTION MUTANT:[[\INST]]"
        "System-Prompt: [SYSTEM_PROMPT]\n" where STOPS_BEFORE(SYSTEM_PROMPT, "\n") and STOPS_BEFORE(SYSTEM_PROMPT, "Task-Prompt:") and STOPS_BEFORE(SYSTEM_PROMPT, "Reasoning-Prompt:") and STOPS_BEFORE(SYSTEM_PROMPT, "  ") and len(TOKENS(SYSTEM_PROMPT)) < 250
        "Task-Prompt: [TASK_PROMPT]\n" where STOPS_BEFORE(TASK_PROMPT, "\n") and STOPS_BEFORE(TASK_PROMPT, "Reasoning-Prompt:") and STOPS_BEFORE(SYSTEM_PROMPT, "  ") and len(TOKENS(TASK_PROMPT)) < 250
        "Reasoning-Prompt: [REASONING_PROMPT]\n" where STOPS_AT(REASONING_PROMPT, "\n") and STOPS_BEFORE(REASONING_PROMPT, "  ") and STOPS_BEFORE(REASONING_PROMPT, "]") and len(TOKENS(REASONING_PROMPT)) < 250
    from
        lmql.model({model_name_or_path}, cuda=True, batch_size={batch_size}, inprocess=True, quantization_config={frozenset(model_kwargs["quantization_config"].__dict__.items())}, dtype="bfloat16", bits=4)
    ''',
    "mutation_lamarckian_args": ["mutation_prompt", "correct_reasonings", "task_prompt"],
################################################
################################################
    "hypermutation_zero_order_direct":
    '''
    {decoder_options}
        "[[INST]] <<SYS>>"
        "You are a prompt engineer creating better prompts for specific tasks. You are to create a prompt to mutate another prompt."
        "<</SYS>>\n\n"

        # THINKING STYLE
        "{thinking_style}"

        "\nTASK DESCRIPTION:\n"

        # TASK DESCRIPTION
        "{task_description}"

        "\n\nMUTANT INSTRUCTION:[[\INST]]"
        "[MUTATION_PROMPT]" where len(TOKENS(MUTATION_PROMPT)) < 250
    from
        lmql.model({model_name_or_path}, cuda=True, batch_size={batch_size}, inprocess=True, quantization_config={frozenset(model_kwargs["quantization_config"].__dict__.items())}, dtype="bfloat16", bits=4)
    ''',
    "hypermutation_zero_order_direct_args": ["thinking_style", "task_description"],
################################################
    "hypermutation_first_order_direct":
    '''
    {decoder_options}
        "[[INST]] <<SYS>>"
        "You are a prompt engineer creating better prompts for specific tasks. You are to create a prompt to mutate another prompt."
        "<</SYS>>\n\n"

        "\nPlease summarize and improve the following instruction:\n"

        # TASK DESCRIPTION
        "{mutation_prompt}"

        "\n\nMUTANT INSTRUCTION:[[\INST]]"
        "[MUTATED_TASK_PROMPT]" where len(TOKENS(MUTATED_TASK_PROMPT)) < 250
    from
        lmql.model({model_name_or_path}, cuda=True, batch_size={batch_size}, inprocess=True, quantization_config={frozenset(model_kwargs["quantization_config"].__dict__.items())}, dtype="bfloat16", bits=4)
    ''',
    "hypermutation_first_order_direct_args": ["mutation_prompt"],
}